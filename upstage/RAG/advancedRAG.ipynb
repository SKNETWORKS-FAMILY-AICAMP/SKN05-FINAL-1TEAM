{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í•„ìš” íŒ¨í‚¤ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kiwipiepy\n",
    "# !pip install --upgrade cohere\n",
    "# !pip install cohere openai langchain pinecone-client\n",
    "# !pip install dill\n",
    "# !pip install rank-bm25\n",
    "# !pip install rank-bm25 nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bm25_retriever_ooo.pkl íŒŒì¼ ë§Œë“œëŠ” ì½”ë“œ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m tokenized_corpus \u001b[38;5;241m=\u001b[39m [word_tokenize(doc\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# ğŸ”¹ BM25 ëª¨ë¸ í•™ìŠµ\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m \u001b[43mBM25Okapi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_corpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# ğŸ”¹ BM25 ëª¨ë¸ ë° ë¬¸ì„œ ID ì €ì¥\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(bm25_pkl_file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pkl_file:\n",
      "File \u001b[1;32mc:\\Users\\USER\\miniconda3\\envs\\20240909\\Lib\\site-packages\\rank_bm25.py:83\u001b[0m, in \u001b[0;36mBM25Okapi.__init__\u001b[1;34m(self, corpus, tokenizer, k1, b, epsilon)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m b\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m=\u001b[39m epsilon\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\miniconda3\\envs\\20240909\\Lib\\site-packages\\rank_bm25.py:27\u001b[0m, in \u001b[0;36mBM25.__init__\u001b[1;34m(self, corpus, tokenizer)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer:\n\u001b[0;32m     25\u001b[0m     corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize_corpus(corpus)\n\u001b[1;32m---> 27\u001b[0m nd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_idf(nd)\n",
      "File \u001b[1;32mc:\\Users\\USER\\miniconda3\\envs\\20240909\\Lib\\site-packages\\rank_bm25.py:52\u001b[0m, in \u001b[0;36mBM25._initialize\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m     48\u001b[0m             nd[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgdl \u001b[38;5;241m=\u001b[39m \u001b[43mnum_doc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_size\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nd\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# NLTK tokenizerë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ í•„ìš”)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ğŸ”¹ JSON íŒŒì¼ ê²½ë¡œ\n",
    "json_file_path = \"../parse/sony_zv-1_list.json\"\n",
    "bm25_pkl_file_path = \"../data/bm25_retrieve_zv-1.pkl\"\n",
    "\n",
    "# ğŸ”¹ JSON íŒŒì¼ ë¡œë“œ\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# ğŸ”¹ BM25 ì¸ë±ìŠ¤ë¥¼ ìœ„í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë° ID ì €ì¥\n",
    "documents = []\n",
    "document_ids = []\n",
    "\n",
    "for page in data:\n",
    "    page_number = page.get(\"page\", \"unknown\")\n",
    "    for item in page.get(\"items\", []):\n",
    "        if \"md\" in item:  # Markdown í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            text_content = item[\"md\"]\n",
    "            documents.append(text_content)\n",
    "            document_ids.append(f\"page-{page_number}-{item.get('value', 'unknown')}\")\n",
    "\n",
    "# ğŸ”¹ ë¬¸ì„œ í† í°í™”\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# ğŸ”¹ BM25 ëª¨ë¸ í•™ìŠµ\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# ğŸ”¹ BM25 ëª¨ë¸ ë° ë¬¸ì„œ ID ì €ì¥\n",
    "with open(bm25_pkl_file_path, \"wb\") as pkl_file:\n",
    "    pickle.dump((bm25, document_ids, documents), pkl_file)\n",
    "\n",
    "print(f\"âœ… BM25 ëª¨ë¸ì´ í¬í•¨ëœ `bm25_retrieve_zv-1.pkl` íŒŒì¼ ìƒì„± ì™„ë£Œ: {bm25_pkl_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# NLTK í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ í•„ìš”)\n",
    "nltk.download('punkt')\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, json_path=None, pkl_path=None):\n",
    "        \"\"\"\n",
    "        JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ BM25 ëª¨ë¸ì„ ìƒì„±í•˜ê±°ë‚˜,\n",
    "        ê¸°ì¡´ BM25 ëª¨ë¸ì„ pkl íŒŒì¼ì—ì„œ ë¡œë“œí•˜ì—¬ ì‚¬ìš©.\n",
    "        \"\"\"\n",
    "        if pkl_path:\n",
    "            self.load_from_pkl(pkl_path)\n",
    "        elif json_path:\n",
    "            self.load_from_json(json_path)\n",
    "        else:\n",
    "            raise ValueError(\"JSON ë˜ëŠ” PKL íŒŒì¼ ê²½ë¡œë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    def load_from_json(self, json_path):\n",
    "        \"\"\"JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ BM25 ëª¨ë¸ì„ ìƒì„±\"\"\"\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        self.documents = []\n",
    "        self.document_ids = []\n",
    "        \n",
    "        for page in data:\n",
    "            page_number = page.get(\"page\", \"unknown\")\n",
    "            for item in page.get(\"items\", []):\n",
    "                if \"md\" in item:\n",
    "                    text_content = item[\"md\"]\n",
    "                    self.documents.append(text_content)\n",
    "                    self.document_ids.append(f\"page-{page_number}-{item.get('value', 'unknown')}\")\n",
    "\n",
    "        self.tokenized_corpus = [word_tokenize(doc.lower()) for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "\n",
    "    def save_to_pkl(self, pkl_path):\n",
    "        \"\"\"BM25 ëª¨ë¸ì„ pkl íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        with open(pkl_path, \"wb\") as pkl_file:\n",
    "            pickle.dump((self.bm25, self.document_ids, self.documents), pkl_file)\n",
    "        print(f\"âœ… BM25 ëª¨ë¸ì´ `{pkl_path}`ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def load_from_pkl(self, pkl_path):\n",
    "        \"\"\"BM25 ëª¨ë¸ì„ pkl íŒŒì¼ì—ì„œ ë¡œë“œ\"\"\"\n",
    "        with open(pkl_path, \"rb\") as pkl_file:\n",
    "            self.bm25, self.document_ids, self.documents = pickle.load(pkl_file)\n",
    "\n",
    "    def retrieve(self, query, top_n=5):\n",
    "        \"\"\"ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ BM25 ê²€ìƒ‰ ìˆ˜í–‰\"\"\"\n",
    "        tokenized_query = word_tokenize(query.lower())\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "\n",
    "        # ìƒìœ„ nê°œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "        top_indexes = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indexes:\n",
    "            results.append({\n",
    "                \"document_id\": self.document_ids[idx],\n",
    "                \"text\": self.documents[idx],\n",
    "                \"score\": scores[idx]\n",
    "            })\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"../parse/sony_zv-1_list.json\"\n",
    "bm25_pkl_path = \"../data/bm25_retrieve_zv-1.pkl\"\n",
    "\n",
    "# BM25 ëª¨ë¸ ìƒì„± ë° ì €ì¥\n",
    "retriever = BM25Retriever(json_path=json_path)\n",
    "retriever.save_to_pkl(bm25_pkl_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# NLTK tokenizerë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ í•„ìš”)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# JSON íŒŒì¼ ê²½ë¡œ\n",
    "json_file_path = \"../parse/sony_zv-1_list.json\"\n",
    "bm25_pkl_file_path = \"../data/bm25_retrieve_zv-1.pkl\"\n",
    "\n",
    "# JSON íŒŒì¼ ë¡œë“œ\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (BM25 ì…ë ¥ìš©)\n",
    "documents = []\n",
    "document_ids = []\n",
    "\n",
    "for page in data:\n",
    "    page_number = page.get(\"page\", \"unknown\")\n",
    "    for item in page.get(\"items\", []):\n",
    "        if \"md\" in item:\n",
    "            text_content = item[\"md\"]\n",
    "            documents.append(text_content)\n",
    "            document_ids.append(f\"page-{page_number}-{item.get('value', 'unknown')}\")\n",
    "\n",
    "# ë¬¸ì„œ í† í°í™”\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# BM25 ëª¨ë¸ í•™ìŠµ\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# BM25 ëª¨ë¸ê³¼ ë¬¸ì„œ IDë¥¼ pkl íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(bm25_pkl_file_path, \"wb\") as pkl_file:\n",
    "    pickle.dump((bm25, document_ids, documents), pkl_file)\n",
    "\n",
    "print(f\"âœ… BM25 ëª¨ë¸ì´ í¬í•¨ëœ `bm25_retrieve_zv-1.pkl` íŒŒì¼ ìƒì„± ì™„ë£Œ: {bm25_pkl_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .pklíŒŒì¼ ë§Œë“œëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JSON ë°ì´í„°ë¥¼ PKL íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ: ../data/zv-1.pkl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# JSON íŒŒì¼ ê²½ë¡œ\n",
    "json_file_path = \"../parse/sony_zv-1_list.json\"\n",
    "pkl_file_path = \"../data/zv-1.pkl\"\n",
    "\n",
    "# JSON íŒŒì¼ ë¡œë“œ\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file: \n",
    "    data = json.load(json_file)\n",
    "\n",
    "# PKL íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(pkl_file_path, \"wb\") as pkl_file:\n",
    "    pickle.dump(data, pkl_file)\n",
    "\n",
    "print(f\"âœ… JSON ë°ì´í„°ë¥¼ PKL íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ: {pkl_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from kiwipiepy import Kiwi\n",
    "import dill\n",
    "\n",
    "kiwi = Kiwi()\n",
    "\n",
    "def kiwi_tokenize(text):\n",
    "    return [token.form for token in kiwi.tokenize(text)]\n",
    "\n",
    "with open(\"../parse/sony_zv-1_list.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "documents = []\n",
    "for item in json_data:\n",
    "    text = item[\"text_field\"]  # \"text_field\"ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” í•„ë“œ ì´ë¦„\n",
    "    documents.append(text)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_texts(documents, preprocess_func=kiwi_tokenize)\n",
    "\n",
    "with open(\"../data/bm25_retriever_a6400.pkl\", \"wb\") as f:\n",
    "    dill.dump(bm25_retriever, f)\n",
    "\n",
    "print(\"bm25_retriever_a6400.pkl íŒŒì¼ ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG ì° ì‹œì‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bm25 pkl íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m document_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m---> 24\u001b[0m     page_number \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmd\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m item \u001b[38;5;129;01mand\u001b[39;00m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip():  \u001b[38;5;66;03m# ë¹ˆ ë¬¸ì„œ í•„í„°ë§\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import dill  # dillì€ pickleë³´ë‹¤ í™•ì¥ëœ ì§ë ¬í™” ì§€ì›\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# NLTK í† í¬ë‚˜ì´ì € ì‚¬ìš©ì„ ìœ„í•œ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ í•„ìš”)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ğŸ”¹ JSON íŒŒì¼ ê²½ë¡œ\n",
    "json_file_path = \"../parse/sony_zv-1_list.json\"\n",
    "bm25_pkl_file_path = \"../data/retriever_zv-1.pkl\"\n",
    "\n",
    "# ğŸ”¹ JSON íŒŒì¼ ë¡œë“œ\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# ğŸ”¹ BM25 í•™ìŠµìš© ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë° ID ì €ì¥\n",
    "documents = []\n",
    "document_ids = []\n",
    "\n",
    "for page in data:\n",
    "    page_number = page.get(\"page\", \"unknown\")\n",
    "    for item in page.get(\"items\", []):\n",
    "        if \"md\" in item and item[\"md\"].strip():  # ë¹ˆ ë¬¸ì„œ í•„í„°ë§\n",
    "            text_content = item[\"md\"]\n",
    "            documents.append(text_content)\n",
    "            document_ids.append(f\"page-{page_number}-{item.get('value', 'unknown')}\")\n",
    "\n",
    "# ğŸ”¹ ë¬¸ì„œ í† í°í™” (BM25 ì…ë ¥ìš©)\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# ğŸ”¹ BM25 ëª¨ë¸ í•™ìŠµ\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# ğŸ”¹ BM25 Retriever ë°ì´í„° ì €ì¥\n",
    "bm25_retriever = {\n",
    "    \"bm25\": bm25,\n",
    "    \"document_ids\": document_ids,\n",
    "    \"documents\": documents\n",
    "}\n",
    "\n",
    "with open(bm25_pkl_file_path, \"wb\") as pkl_file:\n",
    "    dill.dump(bm25_retriever, pkl_file)\n",
    "\n",
    "print(f\"âœ… BM25 Retriever ëª¨ë¸ì´ `{bm25_pkl_file_path}` íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading BM25 retriever: 'list' object has no attribute 'preprocess_func'\n",
      "Warning: BM25 ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´, ë²¡í„° ê²€ìƒ‰ê¸°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings  # Correct importo\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from kiwipiepy import Kiwi\n",
    "from pinecone import Pinecone\n",
    "import cohere\n",
    "import dill  # dill import ì¶”ê°€\n",
    "\n",
    "load_dotenv(override=True)  # ê°•ì œ ë‹¤ì‹œ ë¡œë“œ\n",
    "\n",
    "# API í‚¤ í™•ì¸ ë° ì„¤ì • (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "if not os.environ.get(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter Pinecone API key: \")\n",
    "\n",
    "pinecone_api = os.environ[\"PINECONE_API_KEY\"]\n",
    "cohere_api = os.environ.get(\"COHERE_API_KEY\")  # .get()ì„ ì‚¬ìš©í•˜ì—¬ í‚¤ê°€ ì—†ì„ ë•Œ None ë°˜í™˜\n",
    "\n",
    "if not cohere_api:\n",
    "    raise ValueError(\"COHERE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)\n",
    "pc = Pinecone(api_key=pinecone_api)\n",
    "index_name = \"sony\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # Corrected model name\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": 10},\n",
    ")\n",
    "\n",
    "# Kiwi í† í¬ë‚˜ì´ì € (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)\n",
    "kiwi = Kiwi()\n",
    "def kiwi_tokenize(text):\n",
    "    return [token.form for token in kiwi.tokenize(text)]\n",
    "\n",
    "# BM25 ê²€ìƒ‰ê¸° ë¡œë“œ (ìˆ˜ì •: íŒŒì¼ì—ì„œ ë¡œë“œ)\n",
    "try:\n",
    "    with open(\"../data/zv-1.pkl\", \"rb\") as f:\n",
    "        bm25_retriever = dill.load(f)\n",
    "    bm25_retriever.preprocess_func = kiwi_tokenize\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: zv-1.pkl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. BM25 ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    bm25_retriever = None  # BM25 ê²€ìƒ‰ê¸° ì‚¬ìš© ì•ˆí•¨\n",
    "except Exception as e:  # ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬\n",
    "    print(f\"Error loading BM25 retriever: {e}\")\n",
    "    bm25_retriever = None \n",
    "\n",
    "# ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„± (ìˆ˜ì •: BM25 ê²€ìƒ‰ê¸° ì¡´ì¬ ì—¬ë¶€ í™•ì¸)\n",
    "if bm25_retriever:  # BM25 ê²€ìƒ‰ê¸°ê°€ ë¡œë“œëœ ê²½ìš°ì—ë§Œ ì•™ìƒë¸” ê²€ìƒ‰ê¸° ìƒì„±\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[retriever, bm25_retriever],\n",
    "        weights=[0.5, 0.5]  # Denseì™€ BM25 ê°ê° 50% ê°€ì¤‘ì¹˜\n",
    "    )\n",
    "else:\n",
    "    ensemble_retriever = retriever  # BM25 ê²€ìƒ‰ê¸° ì—†ì„ ê²½ìš° ë²¡í„° ê²€ìƒ‰ê¸°ë§Œ ì‚¬ìš©\n",
    "    print(\"Warning: BM25 ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´, ë²¡í„° ê²€ìƒ‰ê¸°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "cohere_client = cohere.Client(cohere_api)  # ê¸°ì¡´ ì½”ë“œ ìœ ì§€\n",
    "\n",
    "# ... (ì´í›„ ê²€ìƒ‰ ë° ì²˜ë¦¬ ì½”ë“œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.retrievers import BM25Retriever,EnsembleRetriever\n",
    "from kiwipiepy import Kiwi\n",
    "import cohere\n",
    "# import dill\n",
    "\n",
    "load_dotenv(override=True) # ê°•ì œ ë‹¤ì‹œ ë¡œë“œ\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "if not os.environ.get(\"PINECONE_API_KEY\"):\n",
    "  os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter Pinecone API key: \")\n",
    "\n",
    "pinecone_api = os.environ[\"PINECONE_API_KEY\"]\n",
    "cohere_api = os.environ[\"COHERE_API_KEY\"]\n",
    "\n",
    "# vectorstore load\n",
    "# ë¬¸ì„œ ê²€ìƒ‰ (Hybrid Search: BM25 + Pinecone)\n",
    "# pinecone ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api)\n",
    "\n",
    "index_name = \"sony\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "  search_type=\"similarity\", search_kwargs={\"k\": 10},\n",
    ")\n",
    "\n",
    "# BM25 í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ \n",
    "# âœ… BM25Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "# âœ… í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° kiwipiepyë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”\n",
    "# âœ… BM25 ëª¨ë¸ì„ .pkl íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë¡œë“œ ê°€ëŠ¥ -> drillë¡œ ë¡œë“œ\n",
    "kiwi = Kiwi()\n",
    "def kiwi_tokenize(text):\n",
    "    return [token.form for token in kiwi.tokenize(text)]\n",
    "\n",
    "\n",
    "\n",
    "# === 1. BM25Retrieverì™€ Kiwi ë¡œë“œ ===\n",
    "with open(\"../data/zv-1.pkl\", \"rb\") as f:\n",
    "    bm25_retriever = dill.load(f)\n",
    "\n",
    "\n",
    "bm25_retriever.preprocess_func = kiwi_tokenize\n",
    "\n",
    "# === 3. Ensemble Retriever ìƒì„± ===\n",
    "# BM25 + Pinecone ê²°í•© (Ensemble Search)\n",
    "# ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]  # Denseì™€ BM25 ê°ê° 50% ê°€ì¤‘ì¹˜\n",
    ")\n",
    "\n",
    "cohere_client = cohere.Client(cohere_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #âœ… ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ & ì‘ë‹µ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ì—­í• \n",
    "# #âœ… ê° ë‹¨ê³„ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ë©° ì „ë‹¬ë¨\n",
    "# # state ì½”ë“œ ì •ì˜ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, List\n",
    "\n",
    "# 1. ì§ˆë¬¸ ê´€ë ¨ State\n",
    "class QuestionState(TypedDict):\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    transform_question: Annotated[List[str], \"Transformed queries generated by LLM\"]  # Listë¡œ ë³€ê²½\n",
    "\n",
    "# 2. ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ State\n",
    "class ContextRetrievalState(TypedDict):\n",
    "    ensemble_context: Annotated[str, \"Ensemble Retrieve\"]\n",
    "    multi_context: Annotated[str, \"Multi Query\"]\n",
    "    merge_context: Annotated[str, \"Merge Context\"]\n",
    "    filtered_context: Annotated[str, \"Filtering Context\"]\n",
    "    rerank_context: Annotated[str, \"Reranked Context\"]  # reranked context í˜¹ì€ contextë¡œ í‘œê¸°í•˜ê¸°\n",
    "\n",
    "# 3. ë‹µë³€ ë° ë©”ì‹œì§€ ê´€ë ¨ State\n",
    "class AnswerState(TypedDict):\n",
    "    answer: Annotated[str, \"Answer\"]\n",
    "    message: Annotated[List[dict], \"Messages\"] # add_messages type hint ì œê±°. List[dict]ë¡œ ëª…ì‹œ\n",
    "\n",
    "\n",
    "# 4. ì „ì²´ Graph State (ê° ë¶€ë¶„ Stateë¥¼ í¬í•¨)\n",
    "class GraphState(TypedDict): \n",
    "    question_state: QuestionState\n",
    "    context_retrieval_state: ContextRetrievalState\n",
    "    answer_state: AnswerState\n",
    "\n",
    "\n",
    "# # ì‚¬ìš© ì˜ˆì‹œ:\n",
    "# initial_state: GraphState = {\n",
    "#     \"question_state\": {\n",
    "#         \"question\": \"What is the capital of France?\",\n",
    "#         \"transform_question\": []  # ì´ˆê¸° ë¹ˆ ë¦¬ìŠ¤íŠ¸\n",
    "#     },\n",
    "#     \"context_retrieval_state\": {\n",
    "#         \"ensemble_context\": \"\",\n",
    "#         \"multi_context\": \"\",\n",
    "#         \"merge_context\": \"\",\n",
    "#         \"filtered_context\": \"\",\n",
    "#         \"rerank_context\": \"\"\n",
    "#     },\n",
    "#     \"answer_state\": {\n",
    "#         \"answer\": \"\",\n",
    "#         \"message\": [] # ì´ˆê¸° ë¹ˆ ë¦¬ìŠ¤íŠ¸\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "# # ìƒíƒœ ì—…ë°ì´íŠ¸ ì˜ˆì‹œ:\n",
    "# updated_state: GraphState = initial_state.copy() # ì¤‘ìš”: copy()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ìƒíƒœë¥¼ ë³€ê²½í•˜ì§€ ì•Šë„ë¡ í•¨\n",
    "# updated_state[\"question_state\"][\"transform_question\"].append(\"What is the capital of France?\")\n",
    "# updated_state[\"context_retrieval_state\"][\"ensemble_context\"] = \"Paris is the capital...\"\n",
    "# updated_state[\"answer_state\"][\"answer\"] = \"Paris\"\n",
    "# updated_state[\"answer_state\"][\"message\"].append({\"role\": \"assistant\", \"content\": \"Paris is the capital of France.\"})\n",
    "\n",
    "# print(updated_state)\n",
    "\n",
    "\n",
    "# # add_message í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ (ë³„ë„ í•¨ìˆ˜ë¡œ ì •ì˜)\n",
    "# def add_message(state: GraphState, role: str, content: str):\n",
    "#     new_message = {\"role\": role, \"content\": content}\n",
    "#     updated_state = state.copy()\n",
    "#     updated_state[\"answer_state\"][\"message\"].append(new_message)\n",
    "#     return updated_state\n",
    "\n",
    "# updated_state = add_message(updated_state, \"user\", \"Thank you!\")\n",
    "# print(updated_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, List\n",
    "import retriever\n",
    "import ensemble_retriever\n",
    "import bm25_retriever\n",
    "\n",
    "# ğŸ”¹ ì§ˆë¬¸ ê´€ë ¨ State\n",
    "class QuestionState(TypedDict):\n",
    "    question: Annotated[str, \"Original question from user\"]\n",
    "    transform_question: Annotated[List[str], \"Transformed queries generated by LLM\"]  # List íƒ€ì… ìœ ì§€\n",
    "\n",
    "# ğŸ”¹ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ State\n",
    "class ContextRetrievalState(TypedDict):\n",
    "    ensemble_context: Annotated[str, \"Ensemble Retrieved Context\"]\n",
    "    multi_context: Annotated[str, \"Multi Query Retrieved Context\"]\n",
    "    merge_context: Annotated[str, \"Merged Context\"]\n",
    "    filtered_context: Annotated[str, \"Filtered Context\"]\n",
    "    rerank_context: Annotated[str, \"Reranked Context\"]  # í‘œì¤€í™”ëœ key ì´ë¦„ ìœ ì§€\n",
    "\n",
    "# ğŸ”¹ ë‹µë³€ ë° ë©”ì‹œì§€ ê´€ë ¨ State\n",
    "class AnswerState(TypedDict):\n",
    "    answer: Annotated[str, \"Final Answer\"]\n",
    "    message: Annotated[List[dict], \"Message history\"]  # List[dict]ë¡œ ë³€ê²½\n",
    "\n",
    "# ğŸ”¹ ì „ì²´ Graph State (ê° State í¬í•¨)\n",
    "class GraphState(TypedDict):\n",
    "    question_state: QuestionState\n",
    "    context_retrieval_state: ContextRetrievalState\n",
    "    answer_state: AnswerState\n",
    "\n",
    "\n",
    "# ğŸ”¹ **ê¸°ë³¸ Retrieverë¥¼ ì´ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰**\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    ê¸°ë³¸ Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    question = state[\"question_state\"][\"question\"]  # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    documents = retriever.invoke(question)  # ê¸°ë³¸ retriever í˜¸ì¶œ\n",
    "    print(f\"ğŸ” Retrieved documents: {documents}\")\n",
    "\n",
    "    # ì—…ë°ì´íŠ¸ëœ State ë°˜í™˜\n",
    "    state[\"context_retrieval_state\"][\"multi_context\"] = documents  # multi_queryì—ì„œ ê°€ì ¸ì˜¨ ì»¨í…ìŠ¤íŠ¸\n",
    "    return state\n",
    "\n",
    "\n",
    "# ğŸ”¹ **Ensemble Retrieverë¥¼ ì´ìš©í•œ ê²€ìƒ‰**\n",
    "def ensemble_document(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Ensemble Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    question = state[\"question_state\"][\"question\"]  # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    documents = ensemble_retriever.invoke(question)  # Ensemble retriever í˜¸ì¶œ\n",
    "    print(f\"ğŸ” Ensemble Retrieved documents: {documents}\")\n",
    "\n",
    "    # ì—…ë°ì´íŠ¸ëœ State ë°˜í™˜\n",
    "    state[\"context_retrieval_state\"][\"ensemble_context\"] = documents  # Ensemble retrieval ì ìš© ê²°ê³¼ ì €ì¥\n",
    "    return state\n",
    "\n",
    "\n",
    "# ğŸ”¹ **BM25 Retrieverë¥¼ ì´ìš©í•œ ê²€ìƒ‰ (ì˜µì…˜)**\n",
    "def bm25_document(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    BM25 ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    question = state[\"question_state\"][\"question\"]  # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    if bm25_retriever:\n",
    "        documents = bm25_retriever.invoke(question)  # BM25 retriever í˜¸ì¶œ\n",
    "        print(f\"ğŸ” BM25 Retrieved documents: {documents}\")\n",
    "\n",
    "        # ì—…ë°ì´íŠ¸ëœ State ë°˜í™˜\n",
    "        state[\"context_retrieval_state\"][\"filtered_context\"] = documents  # BM25 ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì €ì¥\n",
    "    else:\n",
    "        print(\"âš ï¸ BM25 Retrieverê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.\")\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ContextRetrievalState\n",
    "# import retriever, ensemble_retriever, bm25_retriever\n",
    "\n",
    "# def retrieve_document(state: GraphState) -> GraphState:\n",
    "#     questions = state[\"question\"]\n",
    "#     documents = retriever.invoke(questions)\n",
    "#     print(documents)\n",
    "#     return {\"context\": documents, \"question\": questions}\n",
    "\n",
    "# # Ensemble retriever ì •ì˜\n",
    "# def ensemble_document(state: GraphState) -> GraphState:\n",
    "#     questions = state[\"question\"]\n",
    "#     documents = ensemble_retriever.invoke(questions)\n",
    "#     # print(documents)\n",
    "#     return {\"ensemble_context\": documents}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (API í‚¤ë¥¼ .env íŒŒì¼ì— ì €ì¥í•˜ê³  ë¡œë“œí•©ë‹ˆë‹¤.)\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API í‚¤ í™•ì¸ ë° ì„¤ì •\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# OpenAI Embeddings ì´ˆê¸°í™” (API í‚¤ í¬í•¨)\n",
    "embed_model = OpenAIEmbeddings(openai_api_key=openai_api_key, model=\"text-embedding-ada-002\") # ëª¨ë¸ ì§€ì • (ì„ íƒì )\n",
    "\n",
    "# Pinecone ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” (index ê°ì²´ê°€ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.)\n",
    "pinecone_retriever = Pinecone(index, embed_model)\n",
    "\n",
    "# BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” (documents ë¦¬ìŠ¤íŠ¸ê°€ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.)\n",
    "bm25_retriever = BM25Retriever.from_texts(documents)\n",
    "\n",
    "# Hybrid ê²€ìƒ‰ (BM25 + Pinecone)\n",
    "query = \"Sony ì¹´ë©”ë¼ì˜ í¬ê¸°\"\n",
    "bm25_results = bm25_retriever.get_relevant_documents(query)\n",
    "pinecone_results = pinecone_retriever.similarity_search(query, top_k=5)\n",
    "\n",
    "# ë‘ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ Hybrid ê²€ìƒ‰ ê²°ê³¼ ìƒì„± (ì¤‘ë³µ ì œê±°)\n",
    "combined_results = list(set(bm25_results + pinecone_results))  # ì¤‘ë³µ ì œê±° ë° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(\" Hybrid ê²€ìƒ‰ ê²°ê³¼:\") \n",
    "for doc in combined_results:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.retrievers import BM25Retriever\n",
    "# from langchain.vectorstores import BM25Retriever\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ”¹ Pinecone ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”\n",
    "pinecone_retriever = Pinecone(index, embed_model)\n",
    "\n",
    "# ğŸ”¹ BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”\n",
    "bm25_retriever = BM25Retriever.from_texts(documents)\n",
    "\n",
    "# ğŸ”¹ Hybrid ê²€ìƒ‰ (BM25 + Pinecone)\n",
    "query = \"Sony ì¹´ë©”ë¼ì˜ í¬ê¸°\"\n",
    "bm25_results = bm25_retriever.get_relevant_documents(query)\n",
    "pinecone_results = pinecone_retriever.similarity_search(query, top_k=5)\n",
    "\n",
    "# ğŸ”¹ ë‘ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ Hybrid ê²€ìƒ‰ ê²°ê³¼ ìƒì„±\n",
    "combined_results = bm25_results + pinecone_results\n",
    "\n",
    "# ğŸ”¹ ì¶œë ¥\n",
    "print(\"ğŸ” Hybrid ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "for doc in combined_results:\n",
    "    print(f\"- {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cohere reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from state import GraphState\n",
    "# from ingestion import cohere_client\n",
    "\n",
    "\n",
    "def rerank_with_cohere(query, retrieved_docs, top_n=5):\n",
    "    # Cohereì— ì „ë‹¬í•  ë¬¸ì„œ í˜•ì‹\n",
    "    documents = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    # Reranker í˜¸ì¶œa\n",
    "    \n",
    "    response = cohere_client.rerank(\n",
    "        query=query,\n",
    "        documents=documents,\n",
    "        top_n=top_n,  # ìƒìœ„ Nê°œ ë¬¸ì„œ ì„ íƒ\n",
    "        model=\"rerank-v3.5\"  # ì‚¬ìš©í•  Cohere Reranker ëª¨ë¸\n",
    "    )\n",
    "    \n",
    "    # ìƒìœ„ ë¬¸ì„œë§Œ ë°˜í™˜\n",
    "    reranked_docs = [retrieved_docs[result.index] for result in response.results]\n",
    "    return reranked_docs\n",
    "\n",
    "# Reranker Node\n",
    "def rerank_docs(state: GraphState) -> GraphState:\n",
    "    \n",
    "    questions = state['question']\n",
    "    documents = state['merge_context']\n",
    "    reranked_docs = rerank_with_cohere(questions, documents)\n",
    "    return {\"rerank_context\": reranked_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation.py\n",
    "# ë‹µë³€ ìƒì„± ì²´ì¸\n",
    "from dotenv import load_dotenv\n",
    "# from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "ANSWER_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\",\"context\"],\n",
    "    template=\"\"\"\n",
    "ë‹¹ì‹ ì€ ì¹´ë©”ë¼ ì‚¬ìš©ì ë©”ë‰´ì–¼ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ë©´, ì œê³µëœ Document í˜•ì‹ì˜ contextë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”. ê° Documentì—ëŠ” ì´ë¯¸ì§€ ê²½ë¡œê°€ í¬í•¨ëœ metadataê°€ ìˆìŠµë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ë•Œ, ê´€ë ¨ëœ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° [image: metadata ë‚´ ì´ë¯¸ì§€ ê²½ë¡œ] í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì— í¬í•¨ì‹œì¼œ ì£¼ì„¸ìš”. \n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "ì‚¬ìš©ì ì§ˆë¬¸: \"ì¹´ë©”ë¼ì˜ ISO ì„¤ì • ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.\"\n",
    "ë‹µë³€: \"ì¹´ë©”ë¼ì˜ ISO ì„¤ì •ì€ ë©”ë‰´ì—ì„œ 'ì„¤ì •'ì„ ì„ íƒí•œ í›„ 'ISO' ì˜µì…˜ì„ ì„ íƒí•˜ì—¬ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [image: /path/to/iso_setting_image]\"\n",
    "\n",
    "ì´ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:\n",
    "- ì»¨í…ìŠ¤íŠ¸: {context}\n",
    "- ì§ˆë¬¸: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "generation_chain = ANSWER_PROMPT | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # ë©”ì‹œì§€ ì •ì˜(list type ì´ë©° add_messages í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì¶”ê°€)\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM ì •ì˜\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "# ì±—ë´‡ í•¨ìˆ˜ ì •ì˜\n",
    "def chatbot(state: State):\n",
    "    # ë©”ì‹œì§€ í˜¸ì¶œ ë° ë°˜í™˜\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# ë…¸ë“œ ì´ë¦„, í•¨ìˆ˜ í˜¹ì€ callable ê°ì²´ë¥¼ ì¸ìë¡œ ë°›ì•„ ë…¸ë“œë¥¼ ì¶”ê°€\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì‘ ë…¸ë“œì—ì„œ ì±—ë´‡ ë…¸ë“œë¡œì˜ ì—£ì§€ ì¶”ê°€\n",
    "graph_builder.add_edge(START, \"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ì— ì—£ì§€ ì¶”ê°€\n",
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ì¹´ë©”ë¼ IOSê°’ ì„¤ì •ë²• ì•Œë ¤ì¤˜ì¤˜\"\n",
    "\n",
    "# ê·¸ë˜í”„ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°\n",
    "for event in graph.stream({\"messages\": [(\"user\", question)]}):\n",
    "    # ì´ë²¤íŠ¸ ê°’ ì¶œë ¥\n",
    "    for value in event.values():\n",
    "        print(\"Assistant:\", value[\"messages\"][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20240909",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
