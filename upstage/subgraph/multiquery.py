from typing import List
from langchain_core.output_parsers import BaseOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage
from dotenv import load_dotenv
import json

load_dotenv()

# ğŸ”¹ output ì •ì˜
class LineListOutputParser(BaseOutputParser[List[str]]):
    """
    LLM ì‘ë‹µì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” Output Parser.
    """
    def parse(self, text: str) -> List[str]:
        if isinstance(text, AIMessage):
            text = text.content  # AIMessage ê°ì²´ì—ì„œ content ì¶”ì¶œ
        
        try:
            parsed_json = json.loads(text)
            return parsed_json  # JSON ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        except json.JSONDecodeError:
            lines = text.strip().split("\n")
            return list(filter(None, lines))  # ë¹ˆ ì¤„ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

# Output Parser ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
output_parser = LineListOutputParser()

# ğŸ”¹ Query Prompt ì •ì˜
QUERY_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""You are an AI language model assistant. Your task is to generate five 
    different versions of the given user question to retrieve relevant documents from a vector 
    database. By generating multiple perspectives on the user question, your goal is to help
    the user overcome some of the limitations of the distance-based similarity search. 
    Provide these alternative questions in a JSON array format, separated by commas.
    Do not include any additional explanations.
    Original question: {question}
    Output format: ["question1", "question2", "question3", "question4", "question5"]""",
)

# ğŸ”¹ LLM ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(temperature=0, model="gpt-4o") 

# ğŸ”¹ LLM Chain ìƒì„±
llm_chain = QUERY_PROMPT | llm | output_parser


# ğŸ”¹ GraphState ê¸°ë°˜ ë³€í˜• ì¿¼ë¦¬ ìƒì„± í•¨ìˆ˜
def generate_transformed_queries(state: "GraphState") -> "GraphState":
    """
    ì›ë³¸ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë³€í˜•ëœ ë‹¤ì„¯ ê°œì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì—¬ GraphStateì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    print("--- QUERY GENERATION STARTED ---")
    
    query = state["question_state"]["question"]  # ì›ë³¸ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
    transformed_queries = llm_chain.invoke({"question": query})  # LLM í˜¸ì¶œ

    print(f"Generated Queries: {transformed_queries}")

    # State ì—…ë°ì´íŠ¸
    state["question_state"]["transform_question"] = transformed_queries
    return state
